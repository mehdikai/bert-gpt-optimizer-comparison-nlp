# BERT & GPT Optimizer Benchmarking for NLP

This project evaluates and compares multiple optimization algorithms â€” **AdamW**, **LAMB**, **SGD**, and **AdaFactor** â€” with learning rate warmup for fine-tuning transformer-based models (**BERT** and **GPT**) on the **SST-2 sentiment classification** task.

While testing, **SGD consistently yielded poor accuracy and instability** on the SST-2 dataset. To further investigate, we attempted fine-tuning the **GPT model** using **SGD** on the **SQuAD (Stanford Question Answering Dataset)**. Unfortunately, the results confirmed the trend â€” **accuracy remained low**, suggesting that SGD is not well-suited for fine-tuning large transformer models in this context.



## ğŸš€ Project Highlights

- ğŸ“š Models: BERT, GPT (HuggingFace Transformers)
- âš™ï¸ Optimizers: AdamW, LAMB, SGD, AdaFactor (with Warmup)
- ğŸ“Š Evaluation Criteria:
  - Precision (Accuracy)
  - Training Stability
  - Resource Consumption (GPU/Memory/Time)
- ğŸ§ª Dataset: GLUE SST-2 (Stanford Sentiment Treebank v2), SQuAD (Stanford Question Answering Dataset)

## ğŸ§° Requirements

Python 3.8+

+Transformers

+Datasets

+PyTorch

+NumPy, pandas, matplotlib

Install them via:

pip install torch transformers datasets numpy pandas matplotlib

## ğŸ“ How to Run

# Load dataset and tokenizer
from datasets import load_dataset
from transformers import AutoTokenizer

dataset = load_dataset("glue", "sst2")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Then follow each notebook to run fine-tuning with a specific optimizer.

## ğŸ¤ Contributing
Pull requests and issue discussions are welcome. If you'd like to add more optimizers or other datasets, feel free to fork the repo!

## ğŸ“œ License

MIT License Â© 2025 elmahdi elkaissouni
